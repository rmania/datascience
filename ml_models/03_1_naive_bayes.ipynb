{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (Supervised)\n",
    "\n",
    "NB aggregates information using conditional probability with an **assumption of independence among features**. So the presence of a particular feature in a class is unrelated to the presence of any other feature (regardless of any correlations that may exist). These assumptions are often wrong and that why it is Naive but it allows for simple calculations. \n",
    "\n",
    "The Naive Bayes classifier is based on finding functions **describing the probability of belonging to a class given features**. Here comes Bayes Rule:\n",
    "\n",
    "- $P(Y \\rvert X) = \\frac{P(X \\rvert Y) P(Y)}{P(X)}$\n",
    "\n",
    "Classification using Bayes Rule or Posterior = Likelihood * Prior / Scaling Factor\n",
    "\n",
    "- $P(Y \\rvert X)$ posterior is the probability that sample x is of class y given the\n",
    "feature values of x being distributed according to distribution of y and the prior.\n",
    "\n",
    "- $P(X \\rvert Y)$ - Likelihood of data X given class distribution Y. Gaussian distribution (given by _calculate_likelihood)\n",
    "- $P(Y)$ - Prior (given by _calculate_prior)\n",
    "- $P(X)$ - Scales the posterior to make it a proper probability distribution. This term is ignored in this implementation since it doesn't affect which class distribution the sample is most likely to belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, 'helper_functions/')\n",
    "from data_manipulation import train_test_split\n",
    "from ml_preprocessing import MultiColumnLabelEncoder\n",
    "\n",
    "# path and file (for spam classifier files)\n",
    "PATH_TO_TRAIN_MAILS = 'data/train_mails/'\n",
    "PATH_TO_TEST_MAILS = 'data/test_mails/'\n",
    "\n",
    "# path and file (for sklearn example)\n",
    "PATH_TO_DATA = 'data/'\n",
    "FILE = 'mushrooms.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USe NB for spam classifier (a classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word bag: 20601\n",
      "length of word bag after removals: 16962\n",
      "[('order', 1414), ('address', 1299), ('report', 1217), ('mail', 1133), ('language', 1099), ('send', 1080), ('email', 1066), ('program', 1009), ('our', 991), ('list', 946)]\n"
     ]
    }
   ],
   "source": [
    "# create a bag of most comon words in training mails\n",
    "mails = [os.path.join(PATH_TO_TRAIN_MAILS, x) for x in os.listdir(PATH_TO_TRAIN_MAILS)]\n",
    "most_common_words_size = 2000\n",
    "\n",
    "word_lists = []\n",
    "for mail in mails:\n",
    "    with open(mail) as m:\n",
    "        for line in m:\n",
    "            words = line.split()\n",
    "            word_lists += words\n",
    "\n",
    "bag = Counter(word_lists)\n",
    "print ('length of word bag: {}'.format(len(bag)))\n",
    "\n",
    "for item in list(bag.keys()):\n",
    "    if item.isalpha() == False: # alphabetic, at least 1 char\n",
    "        del bag[item]\n",
    "    elif len(item) == 1:\n",
    "        del bag[item]  \n",
    "print ('length of word bag after removals: {}'.format(len(bag)))\n",
    "bag = bag.most_common(most_common_words) # List the n most common sorted elements \n",
    "print (bag[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir, bag):\n",
    "    \"\"\"\n",
    "    generate a label and word frequency matrix\n",
    "    Args:\n",
    "     mail_dir: directory where mail files are stored\n",
    "     bag: bag of most common words\n",
    "    source\n",
    "     https://github.com/savanpatel\n",
    "    \"\"\"\n",
    "    files = [os.path.join(mail_dir,x) for x in os.listdir(mail_dir)]\n",
    "    feature_matrix = np.zeros((len(files), most_common_words_size))\n",
    "    labels = np.zeros(len(files))\n",
    "    \n",
    "    print ('shape feature_matrix {}'.format(feature_matrix.shape))\n",
    "    print ('no labels: {}'.format(len(labels)))\n",
    "    \n",
    "    agg = 0;\n",
    "    doc_id = 0;\n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            for i, line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        word_id = 0\n",
    "                        for i, d in enumerate(bag):\n",
    "                            if d[0] == word:\n",
    "                                word_id = i\n",
    "                                feature_matrix[doc_id, word_id] = words.count(word)\n",
    "            labels[doc_id] = 0;\n",
    "            filepathTokens = fil.split('/')\n",
    "            lastToken = filepathTokens[len(filepathTokens) - 1]\n",
    "            \n",
    "            if lastToken.startswith(\"spmsg\"):\n",
    "                labels[doc_id] = 1;\n",
    "                agg = agg + 1\n",
    "            doc_id = doc_id + 1\n",
    "            \n",
    "    return feature_matrix, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature matrix has size 702 rows (no emails in `PATH_TO_TRAIN_MAILS` dir) and number of columns represent the number of most common words (defined in `most_common_words_size` variable)\n",
    "- labels are target features. 0 for non-spam mails and 1 for spam mails (identified by the 'spmsga' name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape feature_matrix (702, 2000)\n",
      "no labels: 702\n",
      "shape feature_matrix (260, 2000)\n",
      "no labels: 260\n"
     ]
    }
   ],
   "source": [
    "features_matrix, labels = extract_features(PATH_TO_TRAIN_MAILS, bag = bag)\n",
    "test_feature_matrix, test_labels = extract_features(PATH_TO_TEST_MAILS, bag=bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of non-spam mails in training set: 351\n",
      "no of spam mails in training set: 351\n"
     ]
    }
   ],
   "source": [
    "# no target imbalance\n",
    "print ('no of non-spam mails in training set: {}'\\\n",
    "       .format(labels[labels == 0].shape[0]))\n",
    "print ('no of spam mails in training set: {}'\\\n",
    "       .format(labels[labels == 1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "FINISHED classifying. accuracy score : \n",
      "0.9730769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model = GaussianNB()\n",
    "\n",
    "print (\"Training model.\")\n",
    "#train model\n",
    "model.fit(features_matrix, labels)\n",
    "\n",
    "predicted_labels = model.predict(test_feature_matrix)\n",
    "\n",
    "print (\"FINISHED classifying. accuracy score : \")\n",
    "print (accuracy_score(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn implementation (sklearn.naive_bayes.)\n",
    "\n",
    "Sklearn provides 3 alternatives for model training:\n",
    "- **GaussianNB** --> used in classification. Features are assumed having a normal distribution\n",
    "- **MultinomialNB** --> discrete count. F.i “count how often words occur in a doc”, you can think of it as “number of times outcome number $x_i$ is observed over the $n$ trials”\n",
    "- **BernoulliNB** --> useful if your feature vectors are binary. An application could be text classification with bag-of-words models where the 1s and 0s are \"words occurs in the doc\" and \"word does not occur in the doc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_TO_DATA + FILE)\n",
    "\n",
    "# some cleaning\n",
    "df.columns = df.columns.str.replace('-', '_')\n",
    "df = df.replace('?',np.nan)\n",
    "df['stalk_root'] = df['stalk_root'].fillna('u')\n",
    "\n",
    "# Labelenoce all columns\n",
    "df = MultiColumnLabelEncoder().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features, Target \n",
    "X = df.iloc[:, 1:].values \n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5687, 22)\n",
      "X_test shape: (2437, 22)\n",
      "y_train shape: (5687,)\n",
      "y_test shape: (2437,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import naive_bayes_recipes as nbr\n",
    "nb = nbr.NaiveBayes()\n",
    "nb.fit(X=X_train, y = y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points out of total 2437 points : 199, performance 91.83%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Mislabeled points out of total {} points : {}, performance {:05.2f}%\"\n",
    "      .format(X_test.shape[0], (y_test != y_pred).sum(),\n",
    "          100*(1-(y_test != y_pred).sum() / X_test.shape[0])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB \n",
    "gnb = GaussianNB() \n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mislabeled points out of total 2437 points : 203, performance 91.67%\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Mislabeled points out of total {} points : {}, performance {:05.2f}%\"\n",
    "      .format(X_test.shape[0], (y_test != y_pred).sum(),\n",
    "          100*(1-(y_test != y_pred).sum() / X_test.shape[0])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-two-3ed2670f927d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
