{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn Pipelines\n",
    "\n",
    "Work with the 'spooky author identification dataset` that can be found on Kaggle. https://www.kaggle.com/c/spooky-author-identification\n",
    "\n",
    "Objective of the competition:\n",
    "\n",
    "The competition dataset contains text from works of fiction written by spooky authors of the public domain:\n",
    "\n",
    "- Edgar Allan Poe (EAP)\n",
    "- HP Lovecraft (HPL)\n",
    "- Mary Wollstonecraft Shelley (MWS)\n",
    "\n",
    "The objective is to accurately identify the author of the sentences in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diederik/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'helper_functions/')\n",
    "from ml_visuals import pprint_confusion_matrix\n",
    "\n",
    "#paths\n",
    "PATH = 'data/spooky_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train dataset :  19579\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(PATH + 'train.csv')\n",
    "print(\"Number of rows in train dataset : \",df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check class imbalance.. looks good\n",
    "cnt_srs = df['author'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, ax=ax)\n",
    "_ = ax.set(ylabel ='Occurrences', xlabel = 'Author Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author name: EAP\n",
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.\n",
      "The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall.\n",
      "The surcingle hung in ribands from my body.\n",
      "I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so.\n",
      "\n",
      "\n",
      "author name: HPL\n",
      "It never once occurred to me that the fumbling might be a mere mistake.\n",
      "Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.\n",
      "Herbert West needed fresh bodies because his life work was the reanimation of the dead.\n",
      "The farm like grounds extended back very deeply up the hill, almost to Wheaton Street.\n",
      "His facial aspect, too, was remarkable for its maturity; for though he shared his mother's and grandfather's chinlessness, his firm and precociously shaped nose united with the expression of his large, dark, almost Latin eyes to give him an air of quasi adulthood and well nigh preternatural intelligence.\n",
      "\n",
      "\n",
      "author name: MWS\n",
      "How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services.\n",
      "I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me.\n",
      "He shall find that I can feel my injuries; he shall learn to dread my revenge\" A few days after he arrived.\n",
      "He had escaped me, and I must commence a destructive and almost endless journey across the mountainous ices of the ocean, amidst cold that few of the inhabitants could long endure and which I, the native of a genial and sunny climate, could not hope to survive.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped = df.groupby('author')\n",
    "for name, group in grouped:\n",
    "    print ('author name: {}'.format(name))\n",
    "    cnt = 0\n",
    "    for ind, row in group.iterrows():\n",
    "        print (row['text'])\n",
    "        cnt += 1\n",
    "        if cnt == 5:\n",
    "            break\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing and feature engineering\n",
    "\n",
    "We will start with creating meta featues and see how good are they at predicting the spooky authors. The feature list is as follows:\n",
    "\n",
    "- length of sentence\n",
    "- Number number of words\n",
    "- Number number of words w/o stopwords\n",
    "- Average length of the words\n",
    "- number of commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def text_processing(df):\n",
    "    \"\"\"\n",
    "    clean the text corpus for ml purposes and generate features\n",
    "    input\n",
    "     df --> Pandas dataframe\n",
    "    output\n",
    "     Pandas dataframe\n",
    "    \"\"\"\n",
    "    # removing punctuation + lowering \n",
    "    df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    \n",
    "    # length of sentence\n",
    "    df['length'] = df['processed'].apply(lambda x: len(x))\n",
    "    \n",
    "    # number of words\n",
    "    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n",
    "    \n",
    "    # number of words w/o stopwords\n",
    "    df['words_no_stop'] = df['processed']\\\n",
    "    .apply(lambda x: len([t for t in x.split(' ') if t not in stopwords]))\n",
    "    \n",
    "    # average length of words\n",
    "    df['avg_word_length'] = df['processed']\\\n",
    "    .apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopwords]) \\\n",
    "     if len([len(t) for t in x.split(' ') if t not in stopwords]) > 0 else 0)\n",
    "    \n",
    "    # number of commas\n",
    "    df['commas'] = df['text'].apply(lambda x: x.count(','))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = text_processing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn pipelines facilitate commonly occuring steps. First, split the data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (13117, 6)\n",
      "y_train: (13117,)\n",
      "X_test: (6462, 6)\n",
      "y_test: (6462,)\n"
     ]
    }
   ],
   "source": [
    "features = [c for c in df.columns.values if c not in ['id', 'text', 'author']]\n",
    "numeric_features= [c for c in df.columns.values if c  not in ['id','text','author','processed']]\n",
    "target = 'author'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=.33, random_state=10)\n",
    "print('X_train: {}'.format(X_train.shape))\n",
    "print('y_train: {}'.format(y_train.shape))\n",
    "print('X_test: {}'.format(X_test.shape))\n",
    "print('y_test: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution is like any other transformer. You can call `text.fit()` to fit to training data, `text.transform()` to apply it to training data, or `text.fit_transform()` to do both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Pipeline([('selector', TextSelector(key='processed')),\n",
    "                 ('tfidf', TfidfVectorizer(stop_words='english'))])\n",
    "length = Pipeline([('selector', NumberSelector(key='length')),\n",
    "                   ('standard', StandardScaler())])\n",
    "words =  Pipeline([('selector', NumberSelector(key='words')),\n",
    "                   ('standard', StandardScaler())])\n",
    "words_no_stop =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_no_stop')),\n",
    "                ('standard', StandardScaler())])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler())])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_no_stop', words_no_stop),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13117x21422 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 213556 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "p = Pipeline([('features', feats), \n",
    "          ('clf', RandomForestClassifier(random_state=10)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('text', Pipeline(memory=None,\n",
       "     steps=[('selector', TextSelector(key='processed')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='co...stimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=10, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = p.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6346332404828227"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the best pipeline while doing gridsearches is an important part of many pipelines but it is also an expensive operation. The `p.get_params().keys()` shows all available options. Let' s pick some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=10, verbose=0, warm_start=False),\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_impurity_split': None,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 10,\n",
       " 'clf__n_jobs': 1,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': 10,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False,\n",
       " 'features': FeatureUnion(n_jobs=1,\n",
       "        transformer_list=[('text', Pipeline(memory=None,\n",
       "      steps=[('selector', TextSelector(key='processed')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_fe...rSelector(key='commas')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]))],\n",
       "        transformer_weights=None),\n",
       " 'features__avg_word_length': Pipeline(memory=None,\n",
       "      steps=[('selector', NumberSelector(key='avg_word_length')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]),\n",
       " 'features__avg_word_length__memory': None,\n",
       " 'features__avg_word_length__selector': NumberSelector(key='avg_word_length'),\n",
       " 'features__avg_word_length__selector__key': 'avg_word_length',\n",
       " 'features__avg_word_length__standard': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'features__avg_word_length__standard__copy': True,\n",
       " 'features__avg_word_length__standard__with_mean': True,\n",
       " 'features__avg_word_length__standard__with_std': True,\n",
       " 'features__avg_word_length__steps': [('selector',\n",
       "   NumberSelector(key='avg_word_length')),\n",
       "  ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))],\n",
       " 'features__commas': Pipeline(memory=None,\n",
       "      steps=[('selector', NumberSelector(key='commas')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]),\n",
       " 'features__commas__memory': None,\n",
       " 'features__commas__selector': NumberSelector(key='commas'),\n",
       " 'features__commas__selector__key': 'commas',\n",
       " 'features__commas__standard': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'features__commas__standard__copy': True,\n",
       " 'features__commas__standard__with_mean': True,\n",
       " 'features__commas__standard__with_std': True,\n",
       " 'features__commas__steps': [('selector', NumberSelector(key='commas')),\n",
       "  ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))],\n",
       " 'features__length': Pipeline(memory=None,\n",
       "      steps=[('selector', NumberSelector(key='length')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]),\n",
       " 'features__length__memory': None,\n",
       " 'features__length__selector': NumberSelector(key='length'),\n",
       " 'features__length__selector__key': 'length',\n",
       " 'features__length__standard': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'features__length__standard__copy': True,\n",
       " 'features__length__standard__with_mean': True,\n",
       " 'features__length__standard__with_std': True,\n",
       " 'features__length__steps': [('selector', NumberSelector(key='length')),\n",
       "  ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))],\n",
       " 'features__n_jobs': 1,\n",
       " 'features__text': Pipeline(memory=None,\n",
       "      steps=[('selector', TextSelector(key='processed')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), nor...\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None))]),\n",
       " 'features__text__memory': None,\n",
       " 'features__text__selector': TextSelector(key='processed'),\n",
       " 'features__text__selector__key': 'processed',\n",
       " 'features__text__steps': [('selector', TextSelector(key='processed')),\n",
       "  ('tfidf',\n",
       "   TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None))],\n",
       " 'features__text__tfidf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'features__text__tfidf__analyzer': 'word',\n",
       " 'features__text__tfidf__binary': False,\n",
       " 'features__text__tfidf__decode_error': 'strict',\n",
       " 'features__text__tfidf__dtype': numpy.int64,\n",
       " 'features__text__tfidf__encoding': 'utf-8',\n",
       " 'features__text__tfidf__input': 'content',\n",
       " 'features__text__tfidf__lowercase': True,\n",
       " 'features__text__tfidf__max_df': 1.0,\n",
       " 'features__text__tfidf__max_features': None,\n",
       " 'features__text__tfidf__min_df': 1,\n",
       " 'features__text__tfidf__ngram_range': (1, 1),\n",
       " 'features__text__tfidf__norm': 'l2',\n",
       " 'features__text__tfidf__preprocessor': None,\n",
       " 'features__text__tfidf__smooth_idf': True,\n",
       " 'features__text__tfidf__stop_words': 'english',\n",
       " 'features__text__tfidf__strip_accents': None,\n",
       " 'features__text__tfidf__sublinear_tf': False,\n",
       " 'features__text__tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__text__tfidf__tokenizer': None,\n",
       " 'features__text__tfidf__use_idf': True,\n",
       " 'features__text__tfidf__vocabulary': None,\n",
       " 'features__transformer_list': [('text', Pipeline(memory=None,\n",
       "        steps=[('selector', TextSelector(key='processed')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), nor...\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None))])), ('length', Pipeline(memory=None,\n",
       "        steps=[('selector', NumberSelector(key='length')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))])), ('words',\n",
       "   Pipeline(memory=None,\n",
       "        steps=[('selector', NumberSelector(key='words')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))])), ('words_no_stop',\n",
       "   Pipeline(memory=None,\n",
       "        steps=[('selector', NumberSelector(key='words_no_stop')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))])), ('avg_word_length',\n",
       "   Pipeline(memory=None,\n",
       "        steps=[('selector', NumberSelector(key='avg_word_length')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))])), ('commas',\n",
       "   Pipeline(memory=None,\n",
       "        steps=[('selector', NumberSelector(key='commas')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]))],\n",
       " 'features__transformer_weights': None,\n",
       " 'features__words': Pipeline(memory=None,\n",
       "      steps=[('selector', NumberSelector(key='words')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]),\n",
       " 'features__words__memory': None,\n",
       " 'features__words__selector': NumberSelector(key='words'),\n",
       " 'features__words__selector__key': 'words',\n",
       " 'features__words__standard': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'features__words__standard__copy': True,\n",
       " 'features__words__standard__with_mean': True,\n",
       " 'features__words__standard__with_std': True,\n",
       " 'features__words__steps': [('selector', NumberSelector(key='words')),\n",
       "  ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))],\n",
       " 'features__words_no_stop': Pipeline(memory=None,\n",
       "      steps=[('selector', NumberSelector(key='words_no_stop')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]),\n",
       " 'features__words_no_stop__memory': None,\n",
       " 'features__words_no_stop__selector': NumberSelector(key='words_no_stop'),\n",
       " 'features__words_no_stop__selector__key': 'words_no_stop',\n",
       " 'features__words_no_stop__standard': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'features__words_no_stop__standard__copy': True,\n",
       " 'features__words_no_stop__standard__with_mean': True,\n",
       " 'features__words_no_stop__standard__with_std': True,\n",
       " 'features__words_no_stop__steps': [('selector',\n",
       "   NumberSelector(key='words_no_stop')),\n",
       "  ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))],\n",
       " 'memory': None,\n",
       " 'steps': [('features', FeatureUnion(n_jobs=1,\n",
       "          transformer_list=[('text', Pipeline(memory=None,\n",
       "        steps=[('selector', TextSelector(key='processed')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_fe...rSelector(key='commas')), ('standard', StandardScaler(copy=True, with_mean=True, with_std=True))]))],\n",
       "          transformer_weights=None)),\n",
       "  ('clf',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=10, verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__max_depth': [50, 70],\n",
      " 'clf__min_samples_leaf': [1, 2],\n",
      " 'features__text__tfidf__max_df': [0.9, 0.95],\n",
      " 'features__text__tfidf__ngram_range': [(1, 1), (1, 2)],\n",
      " 'features__text__tfidf__norm': ['l1', 'l2']}\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 214.904s\n",
      "\n",
      "Best score: 0.625\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 70\n",
      "\tclf__min_samples_leaf: 2\n",
      "\tfeatures__text__tfidf__max_df: 0.9\n",
      "\tfeatures__text__tfidf__ngram_range: (1, 1)\n",
      "\tfeatures__text__tfidf__norm: 'l1'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "hp = {'features__text__tfidf__max_df': [0.9, 0.95],\n",
    "      'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "      'features__text__tfidf__norm': ['l1', 'l2'],\n",
    "      'clf__max_depth': [50,70],\n",
    "      'clf__min_samples_leaf': [1,2]}\n",
    "\n",
    "grid_search = GridSearchCV(p, hp, cv=5, verbose=1)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in p.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(hp)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(hp.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'clf__max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-009bb9227adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'clf__max_depth'"
     ]
    }
   ],
   "source": [
    "GridSearchCV(p, cv=5, **best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
